{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook contains the python code ran inside of a Databricks Notebook to mount an S3 bucket onto the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially running findspark to locate Spark installation and add it to the Python path.\n",
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark has now been initiated and we can import the necessary functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# URL Processing within Python\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Read the table containing AWS authentication credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path to the Delta table where the credentials are stored\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Reading the delta table into a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Extracting both Access & Secret Keys from the created Spark DataFrame & encoding the Secret Key for security purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of the AWS Access & Secret Keys from the DataFrame\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encoding the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Mounting the S3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the AWS S3 Bucket's name\n",
    "AWS_S3_BUCKET = \"<bucket_name>\"\n",
    "\n",
    "# Create a name for the mount\n",
    "MOUNT_NAME = \"/mnt/<mount_name>\"\n",
    "\n",
    "# Amazon S3 Source URL for accessing the data stored inside the bucket\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "\n",
    "# Mounting the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Checking that the S3 bucket was mounted successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output a DataFrame inclsuive of the .json files\n",
    "# These .json files were the files created when sending via the API from the python script\n",
    "\n",
    "display(dbutils.fs.ls(\"/mnt/<mount_name>/topics/<topic_name>/partition=0/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Further check to display each JSON file into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the block inside the Databricks notebook to not perform checks\n",
    "# This allows for improved performance by skipping unnecessary checks\n",
    "%sql\n",
    "\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Checking the .pin table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in JSON .pin topic.\n",
    "file_location = \"/mnt/0e4a38902653_mount/topics/0e4a38902653.pin/partition=0/*.json\"\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "df_pin = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .load(file_location)\n",
    "\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Checking the .geo table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in JSON .geo topic.\n",
    "file_location = \"/mnt/0e4a38902653_mount/topics/0e4a38902653.geo/partition=0/*.json\"\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "df_geo = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .load(file_location)\n",
    "\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.3: Checking the .user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in JSON .user topic.\n",
    "file_location = \"/mnt/0e4a38902653_mount/topics/0e4a38902653.user/partition=0/*.json\"\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "df_user = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .load(file_location)\n",
    "\n",
    "display(df_user)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinterest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
